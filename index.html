<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Model, Multi-modal LLM, Spatial-Temporal Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Oryx</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-five-sixths">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><img src="static/images/icon.png" alt="Icon" style="width: 100px; height: 100px; vertical-align: middle;"> <span style="color: rgb(247,178,106);">Oryx MLLM</span>: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href='https://github.com/liuzuyan' target="_blank"><font color="#B082C9"><b>Zuyan Liu</b></font></a><sup>1,2,*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ" target="_blank"><font color="#B082C9"><b>Yuhao Dong</b></font></a><sup>2,3,*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://liuziwei7.github.io/" target="_blank"><font color="#B082C9"><b>Ziwei Liu</b></font></a><sup>3</sup>&emsp;
                </span>
                <span class="author-block">
                  <font color="#B082C9"><b>Winston Hu</b></font></a><sup>2</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en" target="_blank"><font color="#B082C9"><b>Jiwen Lu</b></font></a><sup>1</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://raoyongming.github.io/" target="_blank"><font color="#B082C9"><b>Yongming Rao</b></font></a><sup>2,1</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Tsinghua University&emsp;
                      <sup>2</sup>Tencent&emsp;
                      <sup>3</sup>S-Lab, NTU&emsp;
                      <sup>*</sup>Equal Contribution&emsp;
                    </span>
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/xxxx.xxxx.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/collections/THUdyh/oryx-66ebe5d0cfb61a2837a103ff" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Model Checkpoint</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/Oryx-mllm/Oryx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                <span class="link-block">
                  <a href="https://twitter.com/XingyuFu2/status/1781368539213082683" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
                </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs, which is non-optimal for multimodal understanding and inefficient for processing long visual content. To solve the problem, we propose the <b>Oryx</b>, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to <b>seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths</b> with two core designs: 1) a pre-trained OryxViT model that can <b>encode images at any resolution</b> into LLM-friendly visual representations; 2) a dynamic compressor module that <b>supports 1x to 16x compression on visual tokens by request</b>. Thanks to these designs, Oryx accommodates extremely long visual contexts like videos with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve great capabilities in image, video, and 3D multimodal understanding simultaneously. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel examples-->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Examples of Oryx</h2> <br></div>
  </div></section>

<section class="hero is-small">
  <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <video width="100%" controls>
                <source src="static/videos/case0.mp4" type="video/mp4">
              </video>
              <img src="static/images/answer0.png" width="100%"/>
            </div>

           <div class="item">
            <!-- Your image here -->
            <video width="100%" controls>
              <source src="static/videos/case1.mp4" type="video/mp4">
            </video>
            <img src="static/images/answer1.png" width="100%"/>
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <video width="100%" controls>
              <source src="static/videos/case2.mp4" type="video/mp4">
          </video>
          <img src="static/images/answer2.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>BLINK</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <video width="100%" controls>
              <source src="static/videos/case3.mp4" type="video/mp4">
          </video>
          <img src="static/images/answer3.png" height="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>BLINK</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          </div>
        </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">On-Demand Visual Perception</h2>
      <br>
      As illustrated in Figure 1,
optimizing for resolution and compression can lead to greater efficiency and meet practical needs:
high resolution is crucial for text-relevant tasks, while object-level tasks may require only simple
images, some applications may need to summarize extremely long videos while others maintain high
precision for each frame.
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Brief video introduction about <span style="font-weight:bold;">BLINK Benchmark.</span>. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- BLINK Comparison -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Oryx Architecture</h2>
        <!-- <h2 class="title is-3">BLINK Benchmark -- Unique Features of BLINK?</h2> -->
        <h2 class="content has-text-justified">
          Oryx includes: 1) A well-trained visual encoder OryxViT that flexibly modifies the positional embedding layer and
          employs variable-length self-attention to process visual tokens in batches, thereby generating LLM-
          friendly visual representations at native resolutions. 2) Dynamic compression technique that adjusts
          downsampling ratios arbitrarily while fusing the information through a shared projector, thereby
          supporting extremely long inputs with up to 16x downsampling while maintaining the precision
          for low compression without increasing the overall token length.
        </h2>
        <img src="static/images/method.png" height="100%"/>
      </div>
    </div>
  </div>
</section>



<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">General Temporal Understanding</h2>
        <h2 class="content has-text-justified">
          We conduct experiments on four multiple-choice benchmarks and three generation benchmarks comprehensively and report the main score for each dataset. Oryx exhibits superior performance under a wide range of open-sourced video MLLMs.
        </h2>
        <div class="myrow">
          <img src="static/images/results1.png" height="100%"/>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Long-Form Temporal Understanding</h2>
        <h2 class="content has-text-justified">
          We show results on three mainstream longform temporal understanding datasets, each featuring video inputs of tens of minutes in duration. Oryx demonstrates superior performance, achieving state-of-the-art results and surpassing several proprietary models across various benchmarks.
        </h2>
        <div class="myrow">
          <img src="static/images/results2.png" height="100%"/>
        </div>
        
      </div>
    </div>
  </div>
</section>


<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Image Understanding</h2>
        <h2 class="content has-text-justified">
          We conduct 2D spatial understanding tasks on six representative image benchmarks, including general and task-specific benchmarks. Our Oryx model achieves tier-1 performance across a wide range of MLLMs.
        </h2>
        <div class="myrow">
          <img src="static/images/results3.png" height="100%"/>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">3D Spatial Understanding</h2>
        <h2 class="content has-text-justified">
          We use the popular ScanQA dataset and evaluate the relevant scores. We compare the Oryx model with 3D-specific models together with general open-source MLLMs. Oryx excels in 3D spatial understanding tasks, highlighting its versatility across various applications.
        </h2>
        <div class="myrow">
          <img src="static/images/results4.png" height="100%"/>
        </div>
        
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
